# RAG_product


# Архитектура решения 
## Общая схема компонентов 
   * FAISS - библиотека, предоставляющая эффективные алгоритмы для быстрого поиска и кластеризации эмбеддингов
   * LLM(ollama, Mistral AI, GPT-5)- модель, которую будем задавать системе для генерации ответа
   * LangChain - Библиотека, которую будем использовать для построения приложения(RAG) на основе LLM
   * Tg/Chatbot - для фронтенда чтобы можно было показывать результат работы
   * HuggingFace- Возможные источники данных 


## Потока данных
   *  На вход система получает текст вопроса, фрагмент кода или контекст, затем ассистент будет использовать базу на основе ответов со Stack Overflow/Хабр ответов и генерировать релевантные решения, реконмендации или рабочее решение.
Система распознаёт язык кода, выделяет:
      * Kлючевые слова и исключения 
      * Тип проблемы (сетевая, логическая, синтаксическая);
   * Эти данные потом используются для формирования запроса к базе знаний.


## Формирование поискового запроса (Query Building)
   * На основе анализа создаётся текстовый запрос к векторной базе.
Пример запроса, который формирует система:
      * "Python requests.exceptions.ReadTimeout при вызове requests.get — как исправить ошибку таймаута" 
   * Запрос конвертируется в векторное представление (embedding) при помощи модели (например, text-embedding-3-large или SBERT).


## Поиск релевантных решений (Retrieval)
   * Система ищет в векторной базе (например, FAISS, Qdrant / Pinecone) фрагменты ответов из StackOverflow/Хабра, наиболее похожие на embedding запроса. Она получает, скажем, top-20 кандидатов.


## Фильтрация и переоценка (Reranking):
   * Отбрасываются нерелевантные по языку или тегу (например, python).
   * Cross-encoder reranker оценивает, насколько фрагмент действительно отвечает на вопрос.
   * Сохраняются top-3–5 лучших.


## Подготовка контекста для генерации:
   * Теперь эти фрагменты (ответы с StackOverflow/Хабра) подаются в LM (LLM, например GPT-5 / Mistral / Llama).
Каждый фрагмент содержит:
        * текст решения;
        * кодовый пример;
        * метаданные (ID, источник, ссылка, рейтинг).
   * Пример внутреннего контекста (prompt для LLM):
   * Вопрос пользователя:
Почему возникает ошибка requests.exceptions.ReadTimeout и как её исправить?
Найденные источники:
[1] StackOverflow: "Python requests.exceptions.ReadTimeout" — совет увеличить параметр timeout.
        *    Код:
       requests.get(url, timeout=10)
   Ссылка: https://stackoverflow.com/questions/xxxxxx
[2] Хабр: "Проблемы с сетевыми таймаутами в requests" — причина в слишком коротком времени ожидания.
   Ссылка: https://habr.com/ru/articles/yyyyyy
Инструкция:
Объясни пользователю причину ошибки и предложи конкретное решение.
Приведи пример кода и добавь ссылки на источники ([1], [2]).


## Генерация финального ответа (Augmented Generation):
   * LLM теперь синтезирует финальный ответ, используя извлечённые источники, а не придумывая их.
Пример ответа:
        * Ответ:
Ошибка requests.exceptions.ReadTimeout возникает, когда клиент не получает ответ от сервера за время, указанное в параметре timeout.
В вашем коде timeout=0.001 (1 миллисекунда) — это слишком мало даже для локальной сети.

       * Решение:
Увеличьте время ожидания, например:
import requests
r = requests.get('https://api.github.com', timeout=5)
print(r.status_code)
Также убедитесь, что сеть или API действительно доступны.


## Отображение ответа пользователю:
Фронтенд показывает:
   * Итоговый текст ответа;
   * блоки кода (подсвеченные по синтаксису);
   * ссылки на оригинальные источники;
   * кнопку «Показать исходное решение» или «Это помогло».

## Сбор обратной связи и улучшение:
Если разработчик ставит «Не помогло», система:
   * сохраняет этот кейс;
   * может дообучать reranker / retriever;
   * либо предложить альтернативные решения из базы.



